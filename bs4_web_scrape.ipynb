{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "controlled-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-seating",
   "metadata": {},
   "source": [
    "## Web Scraping with BS4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-reconstruction",
   "metadata": {},
   "source": [
    "BS4 is definitely an older library, so its docs are not as robust or well defined as newer or actively maintained libraries like pandas, but a mixture of introspection and the docs will help you to have a pretty good working knowledge of BS4! It still is however, a robust and essential library for web scrapers even if it doesn't have as many features as a large, large, library like pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-accreditation",
   "metadata": {},
   "source": [
    "**Devpost**<br>\n",
    "First example from towardsdatascience.com, page does not return all html elements, even with alternate parser like html.parser. Arbitary examples of scraping information have been substituted to replace it.\n",
    "\n",
    "Tutorial [here](https://towardsdatascience.com/how-to-scrape-any-website-with-python-and-beautiful-soup-bc84e95a3483)<br>\n",
    "Site to scrape [here](https://devpost.com/hackathons?page=2&search=blockchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "indirect-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain html source and parse with beautifulsoup\n",
    "result = requests.get(\n",
    "    \"https://devpost.com/hackathons?page=2&search=blockchain\")\n",
    "soup = BeautifulSoup(result.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "secondary-supply",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HTML parsed by lxml is no longer consistent with what the code on towardsdatascience.com is looking for\n",
    "#print(soup.body.prettify()) #Uncomment to see many divs from the original page are broken or missing\n",
    "featured_challenges = soup.find_all('a', attrs={'data-role': 'featured_challenge'})\n",
    "featured_challenges #Returns no results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-destiny",
   "metadata": {},
   "source": [
    "It would seem that DevPost has either changed their site structure and introduced dynamic elements that the lmxl parser cannot obtain (which contain the data the towardsdatascience tutorial is looking for) or they have changed the name and attributes of their tags so that the naming convention is now different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "royal-rally",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "https://devpost.com\n",
      "https://secure.devpost.com/users/login?ref=top-nav-login\n",
      "https://secure.devpost.com/users/register?ref_content=signup_global_nav&ref_feature=signup&ref_medium=button\n",
      "https://devpost.com/hackathons\n",
      "https://devpost.com/software\n",
      "https://post.devpost.com\n",
      "https://devpost.com\n",
      "https://devpost.com/hackathons\n",
      "https://devpost.com/software\n",
      "https://post.devpost.com\n",
      "https://secure.devpost.com/users/login?ref=top-nav-login\n",
      "https://secure.devpost.com/users/register?ref_content=signup_global_nav&ref_feature=signup&ref_medium=button\n",
      "https://info.devpost.com/about\n",
      "https://info.devpost.com/careers\n",
      "https://info.devpost.com/contact\n",
      "https://help.devpost.com/\n",
      "https://devpost.com/hackathons\n",
      "https://devpost.com/software\n",
      "https://post.devpost.com\n",
      "https://post.devpost.com/app_contest_resources/\n",
      "https://devpost.com/portfolio/redirect?page=projects\n",
      "https://devpost.com/portfolio/redirect?page=hackathons\n",
      "https://devpost.com/settings\n",
      "https://twitter.com/devpost\n",
      "https://www.facebook.com/devposthacks\n",
      "https://www.youtube.com/playlist?list=PLmJ41elIxG7bRbhhCUQun3e4XNeyKc_-o\n",
      "https://info.devpost.com/privacy\n",
      "https://info.devpost.com/ccpa\n",
      "https://info.devpost.com/guidelines\n",
      "https://info.devpost.com/terms\n"
     ]
    }
   ],
   "source": [
    "#Obtain all links in the page and print them out, note the original links for the hackathons we wanted are not listed here\n",
    "for hyperlink in soup.body.find_all('a'): #Return iterable collection of 'a' tags, or hyperlinks\n",
    "    link = hyperlink.get('href')          #Find the actual link in href attr\n",
    "    if link is not None:                  #Print link, but some 'a' tags have no href attr, so avoid printing \"None\" out\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-broadcast",
   "metadata": {},
   "source": [
    "**HelloHappy**<br>\n",
    "This example utilizes a very basic static website to avoid the issues we run into with devpost using dynamic elements with JS variants (I believe devpost uses vue.js). HelloHappy only displays different typefaces from Google with sample text, so it primarily text-based.\n",
    "\n",
    "Site to scrape [here](https://hellohappy.org/beautiful-web-type/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "pleased-kenya",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "herzog\n",
      "bringhurst\n",
      "nietzsche\n",
      "tufte\n",
      "seneca\n",
      "thin\n",
      "nabokov\n",
      "postnormal\n",
      "slogan\n",
      "darwin\n",
      "headline\n",
      "camus\n"
     ]
    }
   ],
   "source": [
    "#Compiling all typeface names\n",
    "result = requests.get(\n",
    "    \"https://hellohappy.org/beautiful-web-type/\")\n",
    "soup = BeautifulSoup(result.content, 'lxml')\n",
    "text_container = soup.find('section', {'id': 'container'})\n",
    "#List comprehension to store all typeface sections, stored in sections titled with class name \"sample\"\n",
    "typeface_names = [sample.attrs['id'] for sample in text_container.find_all('section', class_='sample')]\n",
    "\n",
    "for typeface in typeface_names:\n",
    "    print(typeface)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-invasion",
   "metadata": {},
   "source": [
    "Getting the sample text from a code perspective is easy enough by invoking the `get_text` method, but what we if shift things to an abstracted user perspective?\n",
    "Here, we'll prompt the user for a typeface and consequently bring up the sample text by reverse searching. <br>\n",
    "\n",
    "If we wanted to, we could do away with needing typeface_names, since a `find` or `find_all` query that produces 0 results will return an empty collection, but since we went through the trouble of compiling it, let's check for valid typeface names that way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "first-beatles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What typeface sample text did you want to bring up? thin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the sample text for typeface thin:\n",
      "\n",
      "\n",
      "\n",
      "Unity\n",
      "Rhythm\n",
      "Balance\n",
      "Emphasis\n",
      "Proximity\n",
      "Hierarchy\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = input(\"What typeface sample text did you want to bring up?\")\n",
    "if query not in typeface_names:\n",
    "    print(\"Sorry, I could not find that typeface in the collection!\")\n",
    "else:\n",
    "    print(\"Here is the sample text for typeface \" +\n",
    "          f\"{query}:\\n{text_container.find('section', {'id' : query}).get_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-salvation",
   "metadata": {},
   "source": [
    "An admittedly simple, yet effective form of web scraping for a static webpage!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
